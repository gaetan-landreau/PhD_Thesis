\chapter{Introduction}
\label{chapter:introduction}

%\minitoc
\chapterwithfigures{\nameref*{chapter:introduction}}
%\chapterwithtables{\nameref*{chapter:introduction}}

\ifthenelse{\boolean{skipIntro}}{\endinput}{}

\emph{Seeing the unseen}.

\section{PhD Context}

\subsection{Meero}
Meero is a Software-as-a-Service french startup founded in 2014 that primarily aims to provide \ac{AI}-powered visual enhancement tools and algorithms for businesses across several verticals, from real estate agencies to e-commerce and fashion industries, as well as automotive car dealerships. Meero proposes a wide range of \ac{AI}-based solutions, from sky replacement, virtual staging or object eraser algorithms for the real-estate vertical to background removal or virtual try-on for fashion and e-commerce companies. Regarding its automotive branch, brand as \textit{CarCutter}, Meero offers car dealerships and marketplaces the opportunity to have visually coherent and appealing images. One of its latest product refers as the 360\degree spin, that allow to virtually smoothly turn around a car given a limited set of images. Such a 3D-based application is inherantly covered by \ac{NVS} as soon as unseen viewpoint must be rendered. 

However, fundamental research in computer vision and its conterpart application in industries suffers from a massive gap that needs to be closed: most of academic papers in vision research deal with images that roughly size from $128\times128$ to $1024\times1024$, whereas images from any mobile device now has at least a 2K resolution (up to 4 to 6K for the latest DSLR camera). Even through such claim tends to thin out with latest fundation models and exploding \ac{GPU} compute capabilities, such an image resolution discrepancy prevent, during this thesis, to directly build an image-based \ac{AI} product in industy from an academic vision paper. This thesis somehow tried to filled this gap, mostly by investigating generalizable single-image novel view synthesis architecture, that could thus be non-restricted to a single scene. 

\subsection{Novel View Synthesis}
\subsection{3D reconstruction}
\section{Contributions}
Aiming to perform novel viewpoint synthesis from a single-image is an extremely ill posed-problem since too many details, structures or texture are unobserved on the provided source view. Core problem that thus inherantly rises from the later observation is to find ways, such as efficient pose encoding, structural constraints that bring as many as possible prior information to the \ac{NN}. 

Years before 2023 emergence of \ac{GenAI} and incredibly powerful foundation models \citep{awais2023foundational}, dataset images we dealt with in single-image \ac{NVS} were mostly low resolution, size $128\times128$, as in ShapeNet \citep{ShapeNet}. We tried through this thesis to incorporate, as much as we possibly could, epipolar geometrical constraints and concepts. We were convinced by the same guiding principle for the last three years: \textit{the most fundamental 3D geometric considerations such as epipolar geometry must be explicitly integrated into deep neural networks.}

We tackle the \ac{NVS} problem with several approaches during this thesis, that could be summarized as follow.
\begin{itemize}
      \item \autoref{chapter:epipolarnvs}: \nameref{chapter:epipolarnvs}\\
            We start approaching single-image \ac{NVS} through the prism of camera transformation encoding. Such an information is vital for any \ac{NN} that performs \ac{NVS}, and its integration as a apriori information is far from being trivial. While several approaches exist to feed such intrinsic and extrinsic parameters to a network, we present in this chapter a novel method to encode such a camera transformation, by extensively leveraging on epipolar geometry. The work in this chapter has led to the following conference publication:
            \begin{itemize}
                \item \fullcite{landreau2022epipolarnvs}
            \end{itemize}


      \item \autoref{chapter:epinerf}: \nameref{chapter:epinerf}\\
            We then turn from camera pose encoding to inner 3D constraints consideration in \ac{NeRF}. However, we maintain the epipolar geometry concept in our work and build a feature-based attention mechanism thanks to \ac{NeRF}-based additional network, called \textit{NeRFeature}. Such a mecanism is direclty involved at training time, while we \textit{do not} have access to the target view to build epipolar constraints. Our work is currently in submission:
            \begin{itemize}
                  \item \fullcite{landreau2024epinerf}
            \end{itemize}

      \item \autoref{chapter:gausssplat}: \nameref{chapter:gausssplat}\\
            We finally relaxed the main hypothesis we dealt with in these first two apparoches to work with \ac{NVS} in a multiple-images scenario with 3D \ac{GS}. Such work mostly rely on \textit{CarCutter} 
industrial considerations, as the next generation of the current 360\degree spin stabilization. However, if rendering at training locations goes fine, stabilizing camera path to render unobserved viewpoint, and thus create seamless 360\degree car animation. 
\end{itemize}
