\chapter{Real-time $360 ^{\circ} $ cars rendering from multiple images with Gaussian Splatting}
\label{chapter:gausssplat}

\ifthenelse{\boolean{skipGauss}}{\endinput}{}

\section{Introduction}
This final section extends the latest two sections that were mostly focused toward a research academic perspective. Core motivation rather lies here on the desire to apply \ac{NVS} latest advances on an industrial project, where processed images resolution are at least $10\times$ higher than ShapeNet-SRN \citep{chang2015shapenet,sitzmann2019scene} ones ($\sim$ 1-2K against $128\times128$). \ac{NeRF} have benefited over the latest few years from massive improvements against several directions; training time, camera pose requirements, inference speed, drastic model weight reduction \textit{etc}. 
However, \ac{NeRF}-based methods suffers from an intrinsic computational bottleneck at inference time. It requires to query the \ac{NeRF} roughly 500 millions times to generate a squared $2K\times2K$ image. As a direct consequence, one of the most acknowledged state-of-the-art method, termed Mip-NeRF360 \citep{barron2022mip}, is only able to render at < 0.1 \ac{FPS} a 1K image. It would therefore be necessary to deploy a lot of \ac{GPU} resources and engineering refactorization to properly parallelize the source code in order to synthesize novel views at acceptable speed. 

We thus work in this section with a different set of constraints. First concerns the generalizable property we kept in the two previous sections. We do not aim anymore to build or rely on an algorithm that can synthesize a viewpoint from any image belonging to a specific class. We thus rather want to leverage on \textit{per-scene} framework, that would require a novel training as soon as we work with a new scenario. Furthermore and as mentioned earlier, images we are now working with are closer to what clients could sent to any \ac{AI}-vision SaS company, both regarding resolution and content. Finally, camera poses are no longer available as ground truth since images CarCutter by Meero has to deal with are unposed. They thus must be first estimated using \ac{SFM} techniques (subsection \ref{subsec:gs-sfm}).

Core idea is rather to leverage on the very latest advances in neural rendering, mostly on \ac{GS} ones. \ac{GS} offers an extremely well balanced trade-off between rendering quality and training/inference requirements. Contrary to \ac{NeRF}-based methods, that are therefore often too slow to render high-resolution images, \ac{GS} shows an outrageously 100 \ac{FPS} performance, withtout relying on any neural network architecture. However, the \ac{NVS} paradigm we were used to deal with until now has heavely changed: View synthesis is not anymore infered given a source image and a camera transformation. As soon as \ac{GS} reconstructs the whole 3D scene through a set of 3D coloured gaussians, render a novel view now at a specific viewpoint only require a camera pose information.

%%%%%%%%%%%%%
%%%%%%%%%%%%
We present on the Figure \ref{fig:gs-vs-homography-view3} how one of the view from the original image set as been stabilized by solely leveraging on image perspective wrapping (center image in Figure \ref{fig:gs-vs-homography-view3}). The same rendered view from the learned \ac{GS} scene is depicted on the right side of Figure \ref{fig:gs-vs-homography-view3}. As the entire 3D scene structure has been learned and is explicitly stored as a gaussian point cloud, one can render it at any locations and synthesise the whole car. The shown rendering comes from the vanilla \ac{GS} framework, without any of our modifications.

\begin{figure*}[htb!]
  \center
\includegraphics[width=\linewidth]{images/gaussiansplatting/perspective-vs-gs.png}
\caption{\textbf{Perspective transformation and Gaussian Splatting based stabilisation} Contrary to the homography-based solution, trained GS scene is able to render the whole car. Whereas the car is now properly centred on the view, the perspective-wrapped car is incomplete as the corresponding transformation to apply was too drastic.}
\label{fig:gs-vs-homography-view3}
\end{figure*}

Applying such a transformation to the entire scene, frame by frame, would allow to stabilized the $360^{\circ}$ spin properly. However, as soon as a source view cannot depict the whole car, such an image-based method will fail. 

\section{Related work}

Reconstruct the 3D geometry and appearance of a scene given a set of unposed images is a long standing issue for both computer vision and graphics community. While an impressive variety of approaches exists for addressing this issue; from photogrammetry-based or Structure-From-X techniques (where X could stands for \textit{Texture, Shading, Silhouette} etc) to structured-lights ones, pionner work in such area get considerations for stereo vision \cite{marr1976cooperative} via an iterative cooperative algorithm between two views. 
 
\begin{table}[h!]
  \centering
   \caption{Properties of interest benchmark for $360^{\circ}$ scene stabilization.}
  \begin{tabular}{lcccc}
  \hline
  
    & Complex Reflexions & Occlusions & Rendering time & \\
  \hline
  \hline
  Image warp.  & \xmark & \xmark & \cmark \\
  NeRF & \cmark & \cmark & \xmark\\
  GS  & \xmark & \cmark & \cmark \\
  Ours  & \cmark & \cmark & \cmark \\
  \hline
  \label{tab:gs-comp}
  \end{tabular}
 
  \label{tab:propstab}
\end{table}


[ADD COUPLE OF LINES ON NERF-BASED METHOD, BIBLIOGRAPHIC WORK ETC.]

We thus leverage in this section upon latest advances in neural rendering, and especially on GS-based techniques \cite{kerbl20233d}, to build up a $360^{\circ}$ spin stabilisation pipeline for unbounded scenes. As soon as such a 3D reconstruction system primarily aims to live in a production pipeline for an industrial application, presented methods and experiments extensively focused onto a single scene, that could be one from a car dealership client. 


In summary, our contributions in this section are three-fold: 
\begin{itemize}
    \item Contribution 1 
    \item Contribution 2
    \item Contribution 3
    
\end{itemize}

\section{Background: Gaussian Splatting}

A first complete overview of the underlying concepts behind GS is presented in this section. GS leverages on 3D Gaussian primitives to explicitly model the 3D scene. It has a fully differentiable pipeline that can solely be supervised at 2D-image level. Figure \ref{fig:gs-overview} gives few insights regarding how such a pipeline is built. \newline

\begin{figure*}[htb!]
    \center
  \includegraphics[width=\linewidth]{images/gaussiansplatting/overview_pipeline.png}
  \caption{\textbf{Gaussian Splatting pipeline overview.} Starting from a sparse 3D point cloud, GS is designed on a lightweight and differentiable pipeline that does not involve any deep or shallow neural architectures. \textit{Original illustration comes from the seminal paper}\cite{kerbl20233d} while our modifications are highlighted in \textcolor{red}{red}.}
  \label{fig:gs-overview}
\end{figure*}

We present below the key steps that were defined in the original GS seminal work \cite{kerbl20233d}.


\noindent \textbf{Initialization} From a sparse coloured Structure from Motion (SfM)-3D point cloud, referred as $\mathcal{P}\in\mathbb{R}^{N\times(3+3)}$, an associated set of 3D Gaussians $\{\mathcal{G}_{k}|k=1,...,K\}$ is build. Each primitive $\mathcal{G}_{k} $ has therefore an attached set of learnable attributes, defined in the Table \ref{tab:gauss-param}. 


\begin{table}[h!]
  \centering
   \caption{Per gaussian $\mathcal{G}_{k} $  parameters that need to be optimized.}
  \begin{tabular}{lcc}
  \hline
  Parameter  & Size & Note \\
  \hline
  Position (mean)  $\mu_{k}$ & 3 & 3D vector $(x, y, z)$ \\
  3D Covariance $\Sigma_{k}$ & 7 & 3D vector $(x, y, z)$ + scalar + 3D vector  $(i, j, k)$\\
  Opacity  $o_{k}$ & 1 & scalar \\
  SH coefficients  $c_{k}$ & 48 & 4 bands of SH; $(1+3+5+7)\times3$ \\
  \hline
  Total (per Gaussian)  & 59 & \\
  \hline
  \end{tabular}
 
  \label{tab:gauss-param}
\end{table}

While mean, color and opacity can be optimised without any constraints with gradient-descent based algorithm, $\Sigma_{k}$ has to remain semi-definite positive \footnote{\textit{i.e} $ \forall a \in \mathbb{R}^{3}: a^{T}\Sigma_{k} a \geq 0$} during training to represent a meaningful 3D coviance matrix. As holding this constraint during the optimization would be to complex, $\Sigma_{k}$ is factorized (as an eigendecomposition) in the world coordinate system via with a rotation matrix $O_{k}$ (analytically expressed with 4 quaternions) and a scaling vector $s_{k}$: 

\begin{equation}
    \Sigma_{k} = O_{k}s_{k}s_{k}^{T}O_{k}^{T}
\end{equation}
As any matrix expressed with $A^{T}A$ is always semi-definite positive, $\Sigma_{k}$ is now properly constrained. 

Considering any point \textit{p} in the world 3D space, the geometry of a primitive $\mathcal{G}_{k}$ has an influence over \textit{p} that can be expressed through: 

\begin{equation}
  \mathcal{G}_{k}(p) = \exp(-\frac{1}{2}(p-\mu_{k})^{T}\Sigma_{k}^{-1}(p-\mu_{k}))
\end{equation}

\noindent \textbf{3D Gaussian projection} The 3D Gaussian parameters are optimised during training phase by only leveraging on an image-based supervision signal. The 3D ellipsoids must thus be render on 2D image plane in a differentiable manner and a corresponding 2D mean and covariance need to be derived from $\mu_{k}$ and $\Sigma_{k}$ for each primitive using EWA splatting \cite{zwicker2001ewa}. 

Given any viewpoint we want to render, let's denote corresponding world-to-camera extrinsic matrix $W=[R|t]$ and the intrinsic $K$. The gaussian center $\mu_{k}$ is easily projected in 2D through the perspective projection: 

\begin{equation}
  \begin{bmatrix}
    u \\
    v \\
    z
  \end{bmatrix} = KW\mu_{k} = K(R\mu_{k}+t)
\end{equation}
and the 2D mean $\mu_{k}' = \begin{bmatrix}
  u/z \\
  v/z
\end{bmatrix}$ can be  thus expressed as:
\begin{equation}
  \mu_{k}' = K(W\mu_{k}/(W\mu_{k}^{(z)}))
\end{equation}

Regarding the 3D covariance, $\Sigma_{k}$ is expressed in the camera coordinate system through: 

\begin{equation}
  \Sigma_{k}^{(cc)}= R\Sigma_{k}R^{T}
  \label{eq:gs-3dcov-transfrom}
\end{equation}

and in image/ray space as:
\begin{equation}
  \Sigma_{k}^{(ic)}= J_{k}\Sigma_{c}J_{k}^{T}
  \label{eq:gs-3d2dcov}
\end{equation}

where $J = \partial \mu_{k}' / \partial \mu_{k}$ is the Jacobian of the mean projection formula (\textit{i.e} the affine approximation of the 3D-2D projection).


As explained in \cite{zwicker2001ewa}, one can drop the thrid row and column of $\Sigma_{k}^{(ic)}$ to get a 2D covariance matrix, named $\Sigma_{k}'$. Applying a projective transformation to any 3D gaussian primitive $\mathcal{G}_{k}$ lead to another scaled 2D gaussian primitive, termed $g_{k}$: 

\begin{equation}
  g_{k}(x) = \exp(-\frac{1}{2}(x-\mu_{k}')^{T}\Sigma_{k}'^{-1}(p-\mu_{k}'))
\end{equation}

3DGS seminal work \cite{kerbl20233d} finally performs alpha blending to render a color $\hat{c}(x)$ through: 

\begin{equation}
\label{eq:gs-alpha-blending}
  \hat{c}(x) = \sum_{k=1}^{K}c_{k}\alpha_{k}\prod_{j=1}^{k-1}(1-\alpha_{j})
\end{equation}

\begin{equation}
\label{eq:gs-alpha-def}
  \alpha_{k} = o_{k}g_{k}(x)
\end{equation}

where primitive’s depth order are sorted from 1 to K. \newline

\noindent \textbf{Adaptative Density Control (ADC)} 
As the original SfM point cloud can be extremely sparse, authors from \cite{kerbl20233d} implement both a densification and pruning strategy during training. 

The pruning is quite simple yet effective: any 3D gaussian that has an opacity $\alpha$ below a fixed threshold $\epsilon_{\alpha}$ during training is removed from $\mathcal{G}$. 

The densification strategy requires to both address under-reconstructed areas (where an insufficient number of gaussians are present) and over-reconstructed areas (where gaussians are too large) of the scene. Small gaussians are thus cloned in those under-reconstructed areas whereas large ones are splited into two new gaussians by leveraging on view-space positional gradients, expressed for each gaussian primitive $\mathcal{G}_{k}$ as : 

\begin{equation}
   \nabla_{\mu_{k}}L= ||\frac{\partial L_{\pi}}{\partial \mu_{k}^{\pi}}|| = \sqrt{\left(\frac{\partial L_{\pi}}{\partial \mu_{ndc,x}^{k,\pi}}\right)^{2} + \left(\frac{\partial L_{\pi}}{\partial \mu_{ndc,y}^{k,\pi}}\right)^{2}}
\end{equation}

where $L_{\pi}$ denotes the loss under viewpoint $\pi$. Such a gradient magnitude is tracked and averaged over all the rendered views $\pi \in \Pi$ during 100 iterations. When: 

\begin{equation}
\frac{\sum \limits_{\pi \in \Pi} ||\frac{\partial L_{\pi}}{\partial \mu_{k}^{\pi}}||}{\sum \limits_{\pi \in \Pi} 1} > \tau_{pos}
\label{eq:adc-original}
\end{equation}
holds for gaussian primitive $\mathcal{G}_{k}$, the latter is either splited or cloned depending on its size. Largest eigenvalue of $\Sigma_{k}$ is used to status on $\mathcal{G}_{k}$ size: if such an eigenvalue is above another fixed user defined threshold, primitive is splited, cloned otherwise. \newline

\noindent \textbf{Differentiable rasterizer} As soon as the supervisory signal is 2-dimensional, one can leverage onto a differentiable rasterizer to made such pipeline end-to-end trainable. We provide the key steps of such a tile-based rasterizer: 

\begin{enumerate}
    \item Image screen is divided into $16\times 16$ 
\end{enumerate}

\noindent \textbf{Training} 
Gaussians parameters are learned during training through the vanilla stochastic gradient descent optimization procedure. The corresponding loss function is expressed through: 
\begin{equation}
    \mathcal{L} = (1-\lambda)\mathcal{L}_{1}(I_{GT},\hat{I}) + \lambda \mathcal{L}_{D-SSIM}(I_{GT},\hat{I})
\end{equation}

\section{Our 3D GS-based pipeline.}
We now extensively present in this section the 3D gaussian splatting system we designed for our $360^{\circ}$ spin rendering stabilization problem. We build up our system on top of three distinct strategies, that all consistently improve the overall pipeline. 

\subsection{Viewing Direction}
First bloc we get into consideration on top of the existing GS vanilla framework come from ViewingDirection-GS (VDGS) \cite{malarzgaussian}. Core authors idea is to some extend mix concepts from both NeRF and GS to get the most of both framework, as depicted on Figure \ref{fig:gs-nerf}. 

\begin{figure}[htpb!]
  \centering
  \begin{subfigure}[b]{0.31\linewidth}
    \includegraphics[width=\linewidth]{images/gaussiansplatting/nerf.png}
    \caption{\textbf{NeRF-based}}
    \label{fig:nerf-ray}
  \end{subfigure}
  \quad % Space between the figures
  \begin{subfigure}[b]{0.31\linewidth}
    \includegraphics[width=\linewidth]{images/gaussiansplatting/gs_vanilla.png}
    \caption{\textbf{GS-based}}
    \label{fig:gs-ray}
  \end{subfigure}
  \quad % Space between the figures
  \begin{subfigure}[b]{0.31\linewidth}
    \includegraphics[width=\linewidth]{images/gaussiansplatting/gs.png}
    \caption{\textbf{Ours}}
    \label{fig:gs-ray}
  \end{subfigure}
  \caption{\textbf{Conceptual difference between NeRF and GS.} Neural radiance fields output color and opacity as a function of the viewing direction. GS original code solely encodes view dependency through the spherical harmonics \textit{sh} basis. Our modification allows to also output the opacity value of each primitive as a function of \textbf{d}. }
  \label{fig:gs-nerf}
\end{figure}

The trained GS scene remains expressed as a 3D gaussians cloud but authors introduced an additional Multi-Layer Perceptron (MLP) that is going to output a corrective update on each gaussian opacity. 

Indeed, each primitive has an opacity property but the latter does not depend on the viewing direction \textbf{d} we looking at during rasterization. Introducing such an MLP, termed $\Phi$ therefore aims to alter gaussian transparency in function of this viewing direction: 

\begin{equation}
\Delta o_{k} = \Phi(\Sigma_{k},c_{k},\textbf{d}) 
\end{equation}

so that the final corrected opacity $o_{k}^{corr.}$ is expressed as: 

\begin{equation}
o_{k}^{corr.} = \Delta o_{k}(\textbf{d}) \times o_{k} 
\end{equation}

This novel view-dependency opacity allow the pipeline we build specifically for car scenes to better handle surface light reflections as well as transparent regions on windows.

The alpha-blending equation, introduced in equation (\ref{eq:gs-alpha-blending}) remains unchanged to get the final rendered pixel color.

Such a lightweight MLP $\Phi$ thus highly improve the light reflections and windows transparency issue we often encounter in our car scenes. In-depth quantitative figures as well as visual insights on the usefulness of such an opacity view dependency are presented in detail in section \ref{sec:exp}. 

\subsection{Visual Hull}

We present in this section a complementary way to get another initial point cloud $\mathcal{P}$ to start GS scene training. 


\begin{figure*}[htbp!]
    \center
  \includegraphics[width=\linewidth]{images/gaussiansplatting/image_sift.png}
  \caption{\textbf{SIFT correspondence between 2 closed views} SIFT keypoints detection and matching from COLMAP struggle a lot on such a reflective scene. Only a very limited number of key-points has been properly matched (in bold).}
  \label{fig:sift-colmap}
\end{figure*}

Most of the SIFT keypoints COLMAP-SfM algorithm relies on do not lie on the car surface, as reflections are extremely hard to handle. Such a claim can be seen on Figure \ref{fig:sift-colmap}. Only a very limited number of SIFT points have been properly matched each other on the two views. Such a poor detection and matching performance led to a 
final point cloud that is extremely sparse: $\mathcal{P}$ has only roughly 4K points and most of them do not live on the car surface, as seen on Figure \ref{fig:sfm-colmap-pc}. 

\begin{figure*}[htbp!]
    \center
  \includegraphics[width=.8\linewidth]{images/gaussiansplatting/colmap_sparsePC.png}
  \caption{\textbf{Final SfM point cloud with predicted camera poses} Wheareas 36 views were involved in the SfM-point cloud construction, only few points lie on the car surface we aim to reconstruct in 3D.}
  \label{fig:sfm-colmap-pc}
\end{figure*}


\begin{figure}[htpb!]
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{images/gaussiansplatting/visualhull-idea.png}
    \caption{\textbf{Overview idea}}
    \label{fig:gs-vh-concept}
  \end{subfigure}
  \quad % Space between the figures
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{images/gaussiansplatting/visualhull-res.png}
    \caption{\textbf{Resulting hull}}
    \label{fig:gs-vh-result}
  \end{subfigure}
  \caption{\textbf{Visual hull concept.} A visual hull is here built as the intersection of three 3D bounding cones from the silhouette masks. Such hull is almost convex in case of our car scenario. One can  sample 3D points within the visual hull to build a new dense point cloud.}
  \label{fig:gs-homography-view3}
\end{figure}


We consider a novel point cloud method initialisation by leveraging upon a visual hull, a silhouette-from-motion based-concept. It only requires to get the posed images $\{I_{i},\pi_{i}\}_{i=1}^{N}$ and the corresponding binary silhouette masks $\mathcal{S}$. The Figure \ref{fig:gs-vh-concept} allows to grasp the visual hull concept. 

We present on Algorithm \ref{alg:gs-vh} the pseudo code that allow to generate such a visual hull (VisHull) point cloud. 

\begin{algorithm}[htpb!]
  \caption{Visual hull contruction}\label{alg:gs-vh}
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \SetKwInOut{Parameter}{parameter}
  \Input{Images $\mathcal{I}$, Silhouette mask $\mathcal{S}$}
  \Parameter{Intrinsic and extrinsic camera parameters $\pi=K[R|t]$}
  \Output{Visual hull-based point cloud $\mathcal{P}_{VH}$ } 
  \medskip
  \KwResult{$\mathcal{P}_{VH}$ can be densely sampled}
  \medskip
  $bbox \gets \mathbf{build3DBB}()$ \tcp*[l]{Create a vanilla 3D bounding boxe}
  $P_{3D} \gets \mathbf{sampleDensely}(bbox)$\hspace{.4cm}\textcolor{gray!80}{\# 
    [$N_{pts}$,3]} \tcp*[l]{Sample points in the BB}

   $P_{2D} \gets \mathbf{project}(P_{3D},\pi)$ \hspace{.4cm}\textcolor{gray!80}{\# 
    [$N_{pts}$,2]} \tcp*[l]{Reproject on image plane }
    
    $idx_{valid} \gets \mathbf{keepValidPoints}(P_{2D},\mathcal{M})$ \hspace{.4cm}\textcolor{gray!80}{\# 
    [$N_{pts}$,1]} \tcp*[l]{Boolean. 1 if valid, 0 otherwise}

    $P_{3D}^{(refined)} \gets P_{3D}[idx_{valid}]$ 

    $P_{VH} \gets \mathbf{setRGBcolor}(P_{3D}^{(refined)},P_{2D},idx_{valid},\mathcal{I})$ \tcp*[l]{Set RGB color to the point cloud through bilinear interpolation}
\end{algorithm}

\newpage

\subsection{PixGS ADC}
\label{gs:pixgs-adc}
Our latest improvement of the vanilla GS framework is directly link to the way gaussians are splited and cloned during the ADC procedure. We extensively leverage upon PixGS \cite{zhang2024pixelgs} to change update the densification formulation.  

Original ADC strategy has several inner limitations in its current form: 
\begin{itemize}
    \item Per-pixel gradient involved in $\frac{\partial L_{\pi}}{\partial \mu^{k,\pi}_{ndc,x}}$ and $\frac{\partial L_{\pi}}{\partial \mu^{k,\pi}_{ndc,y}}$  have different directions. It lead to some gradient collision, were magnitude of $\frac{\partial L_{\pi}}{\partial \mu_{k}^{\pi}}$ does not reach densification threshold while it should. 
    \item Properly setting the two threshold involved in the densification process is not intuitive: optimal threshold values might vary from a scene to another one. 
    \item There is no explicit control regarding the total number of gaussians a scene can reach, leading to potential \textit{OOM} issues.
\end{itemize}

Sparse point cloud areas lead to large ellipsoids gaussian during initialisation as soon as axes length are set considering the nearest neighboring points. Large gaussians are thus involved in loss computation from too many viewpoints. However, summing over a large number of viewpoint and  number of pixels gradients that have different direction could lower down the total magnitude ADC is tracking for densification. Such a behaviour is illustrated on Figure \ref{fig:adc-limitation}



\begin{figure*}[htbp!]
    \center
  \includegraphics[width=\linewidth]{images/gaussiansplatting/adc_limitation.png}
  \caption{\textbf{ADC gradient collision} It might occurred that large gaussians ends with low magnitude gradient, preventing the densification to happen.}
  \label{fig:adc-limitation}
\end{figure*}



To ease notation, we discard viewpoint $\pi$ and denote: 


\begin{equation}
\label{eq:grad-inter}
    g_{k,x} = \frac{\partial L}{\partial \mu^{k}_{ndc,x}} = \sum \limits_{j=1}^{m} \frac{\partial L_{j}}{\partial \mu^{k}_{ndc,x}} 
\end{equation}

% \begin{equation}
% \label{eq:grad-inter}
%     g_{k} = \left (\sum \limits_{j=1}^{m} \frac{\partial L_{j}}{\partial \mu^{k}_{ndc,x}} , \sum \limits_{j=1}^{m} \frac{\partial L_{j}}{\partial \mu^{k}_{ndc,y}} \right)
% \end{equation}

% \begin{equation}
% \frac{\sum_{\Pi} ||g_{k}||}{\sum_{\Pi} 1} > \tau_{pos}    
% \end{equation}

% \begin{equation}
% \frac{\sum_{\Pi} \textcolor{red}{m_{k}^{\pi}} ||g_{k}||}{\sum_{\Pi} \textcolor{red}{m_{k}^{\pi}}} > \tau_{pos}    
% \end{equation}
the x-axis gradient, with the per-pixel gradient expression: 

\begin{equation}
\label{eq:perpix-grad}
\frac{\partial L_{j}}{\partial \mu^{k}_{ndc,x}} = \sum \limits_{l=1}^{3} \frac{\partial L_{j}}{\partial c_{l}^{j}}\times \frac{\partial c_{l}^{j}}{\partial \alpha_{k}^{j}} \times \frac{\alpha_{k}^{j}}{\partial \mu^{k}_{ndc,x} }
\end{equation}

where $c_{l}^{j}$ refers to the color of the $l^{th}$ channel of the pixel $j$. The three different partial derivatives from equation (\ref{eq:perpix-grad}) might have different signs, affecting $\frac{\partial L_{j}}{\partial \mu^{k}_{ndc,x}}$  from equation (\ref{eq:grad-inter}). Complete proof can be found in the supplementary material. The more we sum over a large number of pixel, the more likely $\frac{\partial L}{\partial \mu^{k}_{ndc,x}}$ is prone to get closed to 0, preventing $\nabla_{\mu_{k}}L$ to be higher than $\tau_{pos}$ during the iteration cycle, as shown on Figure \ref{fig:adc-limitation}. 

PixGS tackles this gradient collision by re-weighting equation (\ref{eq:adc-original}) with the number of pixels $m_{k}$ covered by each primitives $\mathcal{G}_{k}$: 

\begin{equation}
\frac{\sum \limits_{\pi \in \Pi} m_{k}^{\pi} ||\frac{\partial L_{\pi}}{\partial \mu_{k}}||}{\sum \limits_{\pi \in \Pi}m_{k}^{\pi}} > \tau_{pos}
\label{eq:adc-original}
\end{equation}

PixelGS re-weighting strategy is illustrated on Figure \ref{fig:pixgs-adc}. 



\begin{figure*}[htbp!]
    \center
  \includegraphics[width=\linewidth]{images/gaussiansplatting/pixgs_implem_improvement.png}
  \caption{\textbf{PixGS ADC correction} By accounting on the number of pixel covered by a primitive, large gaussians have a higher gradient that is more likely to reach the density threshold $\tau_{pos}$}
  \label{fig:pixgs-adc}
\end{figure*}



\section{Experiments}
\label{sec:exp}

As our work has to be frame as a potential industrial system, all presented results come from the 36 views scene depicted on Figure \ref{fig:all_views}. 
\begin{figure*}[htpb!]
  \center
\includegraphics[width=.9\linewidth]{images/gaussiansplatting/original_scene.png}
\caption{\textbf{36 views $360^{\circ}$ spin scene.} Images has been acquired with a mobile device at $1500\times 2000$ resolution. As observed, car distance from the device objective is inconsistent across viewpoint.}
\label{fig:all_views}
\end{figure*}

We give visual insights and quantitative figures in this section to highlight how  each of the three modules we added to our GS system consistently improved the rendering quality. 

\subsection{Viewing Direction}

Additional MLP $\Phi$ that was added to our GS pipeline significantly improve transparent areas (such as windows) and highly specular regions, which cars are proned to.  We present a side by side comparison on Figure \ref{fig:gs-vh}. 



\begin{figure}[htb!]
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{images/gaussiansplatting/00023-gs.png}
    \caption{\textbf{Vanilla GS rendering view}}
    \label{fig:view3}
  \end{subfigure}
  \quad % Space between the figures
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\linewidth]{images/gaussiansplatting/00023-gs-vd.png}
    \caption{\textbf{With Viewing Direction MLP.}}
    \label{fig:gs-view3-gs}
  \end{subfigure}
  \caption{\textbf{Viewing Direction influence} Whereas vanilla Gaussian Splatting code struggles with reflective and transparent regions, adding such an MLP before the rasterization significantly improve rendering results. }
  \label{fig:gs-vh}
\end{figure}



\subsection{Visual hull} 

We present on Figure \ref{fig:gs-vishull-comp} three renderings of the same view but from three different trained scene. First scene was trained with the original point cloud procuded by the COLMAP-SfM algorithm, second with the VisHull point cloud while the latest get consideration for merging both COLMAP-SfM and VisHull point cloud together.   



\begin{figure*}[htb!]
  \center
\includegraphics[width=\linewidth]{images/gaussiansplatting/vishull_ablation.png}
\caption{\textbf{Point cloud initilization influence.} COLMAP-based point \textit{(left)},visual hull based \textit{(center)} and merged point clouds \textit{(right)}  rendering result.}
\label{fig:gs-vishull-comp}
\end{figure*}


COLMAP-SfM based rendering yields a satisfying result, even though some inner details are missed. The visual hull based initialization method produce sharper results in the car, but floaters are rendered on the roof the car. Merging the two point cloud \textit{(right)} allows to get the best of both configuration. 

These visual improvement on the rendered views are quantitatively confirmed through the whole training set with the three metrics we used to assess the rendering quality. Results are gathered in Table \ref{table:gs-vh-influence}. Figures tend to highlight a phenomena that was highly expected: as soon as no points are affected in the background regions at the initialisation stage with the \textit{VisualHull} configuration, the latter is weaker than the \textit{COLMAP-SfM} based setting at rendering clear background. Such a limitation is quantatively confirmed in the figures: results from \textit{VisualHull} configuration are better when metrics are only computed on the car. 
\begin{table}[htp!]
  \caption{\textbf{VisualHull influence} Quantitative results how influenciable the original point cloud can be on the GS rendering performance.}
  \label{table:gs-vh-influence}
  \begin{adjustbox}{width=\linewidth}
  \begin{tabular}[h]{c||ccccccc}
  \hline
   PC initialization & \multicolumn{3}{c}{Full Image} & \multicolumn{3}{c}{Car only} \\
   &  SSIM ($\uparrow$) & PSNR ($\uparrow$) & LPIPS ($\downarrow$) & SSIM ($\uparrow$) & PSNR ($\uparrow$)) & LPIPS ($\downarrow$)\\
  \hline
  COLMAP-SfM \textit{only} & 0.808 & 29.163 & 0.318 & 0.983 & 40.917 & 0.031\\
  VisualHull \textit{only} & 0.764 & 27.254 & 0.373 & 0.980 &   39.271 & 0.035 \\
  COLMAP-SfM + VisualHull & \textbf{0.810} & \textbf{29.288} & \textbf{0.314}  & \textbf{0.984} & \textbf{41.249}   & \textbf{0.030} \\
  \hline 
  \end{tabular}
  \end{adjustbox}
  \end{table}


\subsection{PixGS ADC} 

We finally highlight the  positive impact a clever adaptive density control have. Qualitative results are shown in Figure \ref{fig:gs-pigs}. 


\begin{figure}[htb!]
  \centering
  \begin{subfigure}[b]{0.45\linewidth}
    \includegraphics[width=\linewidth]{images/gaussiansplatting/00029-gs.png}
    \caption{\textbf{Vanilla GS rendering view}}
    \label{fig:view3}
  \end{subfigure}
  \quad % Space between the figures
  \begin{subfigure}[b]{0.45\linewidth}
    \includegraphics[width=\linewidth]{images/gaussiansplatting/00029-pixgs.png}
    \caption{\textbf{PixGS rendering view}}
    \label{fig:gs-view3-gs}
  \end{subfigure}
  \caption{\textbf{PixGS Adaptative Density Control influence} Both logo on the rear windows are missing on the vanilla GS version: nor the \textit{Renault elf} and \textit{ECO} sign are visible. PixGS, through its clever densification strategy, managed to render such tiny details.}
  \label{fig:gs-pigs}
\end{figure}

Only core difference PixGS has over the vanilla GS implementation is the ADC component. As soon as such a component is replaced in our system, rendered views immediately gain in sharpness and details: over and under- reconstructed areas are better handle by the ADC module. Nor the viewing direction MLP or the visual hull initialisation were used here. 

\subsection{Complete pipeline}

We finally conduct a complete ablation study over our different modules to emphasize on the consistency each of our modifications bring to the entire GS pipeline system. 

Table \ref{table:gs-abaltion} summarises the impact each of our module has. Whereas both the viewing direction MLP and the PixGS densification strategy steadily enhance all our metrics, the visual hull has only a limited impact outside the car region. Such an observation is in line with what was already observed in figures from Table \ref{table:gs-abaltion}. 

\begin{table}[htp!]
  \caption{\textbf{Ablation study} Influence of our different algorithm components on our $360^{\circ}$ GS system.}
  \label{table:gs-abaltion}
  \begin{adjustbox}{width=\linewidth}
  \begin{tabular}[h]{c||ccccccc}
  \hline
   Pipeline & \multicolumn{3}{c}{Full Image} & \multicolumn{3}{c}{Car only} \\
   &  SSIM ($\uparrow$) & PSNR ($\uparrow$) & LPIPS ($\downarrow$) & SSIM ($\uparrow$) & PSNR ($\uparrow$)) & LPIPS ($\downarrow$)\\
  \hline
  GS \cite{kerbl20233d}  & 0.798  & 28.413 & 0.325 & 0.980 & 38.668 & 0.036 \\
  + Viewing Direction & 0.808 & 29.163 & 0.318 & 0.983 & 40.917  & 0.031 \\
  + VisualHull & 0.810 & 29.288 & 0.314 & 0.984 &41.249  & 0.030 \\
  + PixGS ADC & \textbf{0.861} & \textbf{31.077} & \textbf{0.248} & \textbf{0.990} & \textbf{43.983} & \textbf{0.020} \\
  \hline 
  \end{tabular}
  \end{adjustbox}
  \end{table}

These figures are finally visually confirmed through the exhaustive rendered views depicted on Figure \ref{fig:gs-vh-result}. As one could observed, the original GS code fails on this rendered view to retrive tiny and complex details as the tank fuel or the car interior. Adding the viewing direction MLP $\Phi$ before the rasterizing stage helps the training of our system a lot. Corresponding rendered view has now a better car interior, whereas the fuel tank is still partially rendered. On top of this first modification, merging both the visual hull point cloud with the one produced by COLMAP helped the GS training: car exterior and interior details has been preserved and even improved. The tank fuel is now almost entirely rendered. Finally, the densification strategy proposed by PixGS is striking regarding car details. Fuel tank is perfectly rendered but car interior is now extremely well rendered and one could struggle to distinguish the real ground truth image from the rendered one by only focusing on the car itself.   



\begin{figure*}[htb!]
  \center
\includegraphics[width=\linewidth]{images/gaussiansplatting/image_final.png}
\caption{\textbf{Cumulative influence of our different module} Each module that was implemented in our system consistently bring its own contribution without deteriorating what was improved by former algorithm brick. Car interior and fuel tank details are progressively improved until our final system model.}
\label{fig:gs-vh-result}
\end{figure*}



\section{Conclusion}






