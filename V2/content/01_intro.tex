\chapter{Introduction}
\label{chapter:introduction}

%\minitoc
\chapterwithfigures{\nameref*{chapter:introduction}}
%\chapterwithtables{\nameref*{chapter:introduction}}

\ifthenelse{\boolean{skipIntro}}{\endinput}{}

\emph{Perspicere} - \textit{to see through}. Behind the Perspective's etymology is hidden the notion of portraying our three-dimensional physical reality onto a two-dimensional plane. Such concept has been extensively studied for centuries, and found its oldest fundation in the geometry Euclide defined in his \cite{euclide}. Florence, with its artists and architects, paved the way during Quattrocento in Italian Renaissance of linear perspective studies, to represent as accurately as possible surrounding world on paintings and drawings. Brunelleschi (1377-1446) is one the very first that studied how lines, shapes, objects change under different viewpoint observation, at changing angles. Defined with lines of sights that should converge on one or several vanishing points, linear perspective aims to simulate world objects appearance as a viewer's eye would see them. The fresco from the Italian Renaissance painter Pietro Perugino depicted on Figure \ref{fig:intro_perugino} demonstrates how linear perspective, with precise geometric proportions, can instantaneously create an illlusion of depth on a flat surface. 

\begin{figure}[h!]
      \begin{center}
      \includegraphics[width=.8\textwidth]{images/introduction/perugino.jpg}
      \end{center}
      \caption{\textit{The Delivery of the Keys}, 1481–1482, Sistine Chapel, Rome by Perugino (1481–1482). This impressive $3.3\times 5.5$ meters fresco that both illustrated linear perspective and Brunelleschi's architectural style. The central vanishing point, placed behind the Christ, guides the viewer's gaze towards the background structure, thus enhancing the three-dimentionality feeling. }
      \label{fig:intro_perugino}
\end{figure}

Even through artistic perspective studies were extremely well-tuned from a technical and mechanical standpoint \citep{simon2021jan}, perspective found new expressions in sciences few centuries later, through the advent of photogrammetry. Notion speaks for itself when we get back at its grec ethymology, \textit{photo} (\ie light), \textit{gramma} (\ie drawing, writing) and \textit{metron} (\ie measure). Aimé Laussedat, a French astronomer, geodesist, surveyor and cartographer used the \textit{Hôtel des Invalides} in 1849 to observe, measure and thus try to reproduce physical spaces, lines and objects from multiples perspective views. An illustration of a technical drawing he made for the Vincennes castle in 1850 is presented on Figure \ref{fig:intro_laussedat}. A corresponding  Photogrammetry therefore leverages parallax effect to extract depth and dimensions from our physical world with observed views and was intensively used during mid last century for military purposes. The advent of aerial photography, enabled by recent advancements in aviation, allowed for the topographic mapping of entire countries during the Interwar period.

\begin{figure}[h!]
      \begin{center}
      \includegraphics[width=.5\textwidth]{images/introduction/laussedat_phtograpmetrie.png}
      \end{center}
      \caption{Surveyed by the method of graphical intersections applied to perspectives recorded with the camera lucida. Survey of the Château de Vincennes by A. Laussedat, 1850}
      \label{fig:intro_laussedat}
\end{figure}

Photogrammetry has finally been heavily studied through the prism of robotics and computer vision during 1980's, with increasing computational power and emerging digital imaging technology. Structure-from-Motion approaches naturally arised from such an increasingly strong convergence between photogrammetry and computer vision in the meantime, via pioneered work from Shimon Ullman \citep{ullman1979interpretation}. Such a domain paved the way on novel view synthesis issues (and 3D reconstruction) by filling the gap between photographic scenes capturing processes and their comprehensive three-dimensional representation. 

\ac{AI}, in its commonly broad, unclear, and somewhat disputed definition within society, has a suprisingly more than a 70 years background history, with Alan Turning as the most ackwolegded earliest founding fathers of computer science \citep{turing1950computing} and artificial intelligence with John McCarthy. \ac{AI} is defined by \href{https://www.britannica.com/technology/artificial-intelligence}{The Encyclopedia Britannica} as \textit{"the ability of a computer to perform tasks commonly associated with intelligent beings"}. In the realm of Computer Vision and Graphics, this manuscript frames in a shrinked domain of \ac{AI}, commonly refered as \ac{DL}: \ac{DNN} are thus going to be considered in a large extend in the next few pages... 

\ac{DL} has itself a pationate and outstanding past history, whose one of most recognizable figures today in vision community is Yann LeCun. He has now been working over the last 30 years on vision \ac{AI} considerations, and notably introduced \ac{CNN} \citep{lecun1998gradient}, that have been widely used over the last three years of this thesis. The release in 2009 of ImageNet \citep{deng2009imagenet}, a million annotated images database, and the \ac{CNN} based image-classifier AlexNet \citep{krizhevsky2012imagenet} in 2012 are unanimously seen as the \ac{DL} breaktrhought debut. Vision tasks that were tackled by such \ac{DL} algorithms quickly grows in complexity, from segmentation \citep{long2015fully} to detection \citep{girshick2015fast} or image generation model \ac{GAN} \citep{goodfellow2014generative}. Whereas image-based vision issues have thus seen substantial gains over the last decade, the last third dimension to adress 3D issues from our surrounding world has been added and considered since only few years, driven by latest \ac{GPU} computing advances. Very latest deep architecture are thus now able to generate a high-quality and textured 3D mesh from a single image. An output produced by InstantMesh \citep{xu2024instantmesh}, one of the latest state-of-the-art single-image 3D reconstruction work, is shown on Figure \ref{fig:intro-instantmesh}. Through 3D geometrical prior integration considerations for novel view synthesis, this thesis thereby finds its place within the premises of what 3D AI, on the verge of a major growth in the forthcoming years, represents today.  


\begin{figure}[htp!]
      \centering
      \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=.7\linewidth]{images/introduction/input-instantmesh.png}
        \caption{Input image}
      \end{subfigure}
      \hfill
      \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{images/introduction/output-instantmesh.png}
        \caption{Novel views}
      \end{subfigure}
      \caption{\textbf{InstantMesh results.} (a) InstantMesh takes as input a single image of an unique object set on a white background. (b) Novel views from the car can be obtained by rendering the produced 3D mesh.}
    \label{fig:intro-instantmesh}
    \end{figure}



\section{PhD Context}

\subsection{Meero}
Meero is a Software-as-a-Service french startup founded in 2014 that primarily aims to provide \ac{AI}-powered visual enhancement tools and algorithms for businesses across several branches, from real estate agencies to e-commerce and fashion industries, as well as automotive car dealerships. Meero proposes a wide range of \ac{AI}-based solutions, from sky replacement, virtual staging or object eraser algorithms for the real-estate vertical to background removal or virtual try-on for fashion and e-commerce companies. Regarding its automotive branch, brand as \textit{CarCutter}, Meero offers car dealerships and marketplaces the opportunity to have visually coherent and appealing images. One of its latest product refers as the 360\degree spin, that allow to virtually smoothly turn around a car given a limited set of images. Such a 3D-based application is inherantly covered by \ac{NVS} as soon as unseen viewpoint must be rendered. 

However, fundamental research in computer vision and its conterpart application in industries suffers from a massive gap that needs to be closed: most of academic papers in vision research deal with images that roughly size from $128\times128$ to $1024\times1024$ pixels, whereas images from any mobile device now have at least a 2K resolution (up to 4 to 6K for the latest \ac{DSLR}). Even through such claim tends to thin out with latest fundation models and exploding \ac{GPU} compute capabilities, such an image resolution discrepancy prevent, during this thesis, to directly build an image-based \ac{AI} product in industy from an academic vision paper. This thesis somehow tried to filled this gap, mostly by investigating generalizable single-image novel view synthesis architecture, that could thus be non-restricted to a single scene. 

\subsection{3D reconstruction}
 \ac{NVS} is somehow inherently intertwined with 3D reconstruction, as synthesis of novel views was historically de facto allowed once the complete 3D representation was acquired. While an impressive variety of approaches exists for addressing this issue; from photogrammetry-based or Structure-From-X techniques (where X could stands for \textit{Motion} \citep{longuet1981computer}, \textit{Shading} \citep{horn1989obtaining}, \textit{Silhouette}\citep{baumgart1974geometric} etc) to structured-lights ones, pionner work in such area get considerations for stereo vision \citep{marr1976cooperative} via an iterative cooperative algorithm between two views. 
 
 First \ac{AI}-based 3D reconstruction algorithms started to emerge from 2018 with seminal research from \citep{kato2018neural} that introduces one of the first differentiable neural renderer into a deep learning pipeline.  The release in 2020 of PyTorch3D \citep{ravi2020pytorch3d} for \ac{DL}-based 3D code development has played a significant role in the widespread adoption of 3D issues among the scientific community. From multi-view \citep{li2023neuralangelo} to even single-image \citep{voleti2024sv3d} 3D reconstruction, latest groundbreaking advances in 2023/2024 herald an exciting future for research in this field. 

3D reconstruction and \ac{NVS} meets each other in this thesis through the \autoref{chapter:gausssplat}, where a 3D scene needs first to be explicitly reconstructed before being able to render any novel viewpoint at unseen locations. 

\subsection{Novel View Synthesis}
\ac{NVS} presents a challenging task in computer vision, wherein the objective is to generate an image from an unobserved viewpoint using only a limited number of source images and their corresponding camera pose information. This thesis is focused toward the most difficult scenario, which exclusively relies on a unique source image. In 1998, Thomas Vetter presented a stunning approach \cite{vetter1998synthesis} for single face image \ac{NVS}, where views were used to \textit{learn} pose-invariant descriptors and by leveraging a single 3D human head generic model. \ac{NVS} has naturally started being considered through \ac{AI} prism in 2015 \citep{yang2015weakly}, even though camera displacement were discretized, preventing novel view rendering from any free viewpoint. The branch masively benefited in late 2020 from a simple, yet outstandingly powerful, novel framework concept, referred as \ac{NeRF} \cite{mildenhall2020nerf}. A year ago, \ac{GS} models had once again disrupted the field, allowing novel view synthesis in realtime with 2K  images on a single consumer \ac{GPU} machine. 


\section{Contributions}
Novel viewpoint synthesis from a single-image has several plausible solutions and is thus ill posed-problem. There are too many details, structures or texture that remain unobserved with a single source view. Core problem that thus inherantly rises from the later observation is to find ways, such as efficient pose encoding, structural constraints that bring as many as possible prior information to the \ac{NN}. 

Years before 2023 emergence of fundations models and \ac{GenAI}\footnote{We make a clear distinction with the very first generative networks that were build in 2014 with \ac{GAN} \citep{goodfellow2014generative}} and incredibly powerful foundation models \citep{awais2023foundational}, dataset images we dealt with in single-image \ac{NVS} were mostly low resolution, size $128\times128$, as in ShapeNet \citep{chang2015shapenet}. We tried through this thesis to incorporate, as much as we possibly could, epipolar geometrical constraints. We were convinced by the same guiding principle for the last three years: \textit{the most fundamental 3D geometric considerations such as epipolar geometry must be explicitly integrated into deep neural networks.}

We tackle the \ac{NVS} problem with several approaches during this thesis, that could be summarized as follow.
\begin{itemize}
      \item \autoref{chapter:epipolarnvs}: \nameref{chapter:epipolarnvs}\\
            We start approaching single-image \ac{NVS} through the prism of camera transformation encoding. Such an information is essential for any \ac{NN} that performs \ac{NVS}, and its integration as a apriori information is far from being trivial. While several approaches exist to feed such intrinsic and extrinsic parameters to a network, we present in this chapter a novel method to encode such a camera transformation, by extensively leveraging on epipolar geometry. The work in this chapter has led to the following conference publication:
            \begin{itemize}
                \item \fullcite{landreau2022epipolarnvs}
            \end{itemize}


      \item \autoref{chapter:epinerf}: \nameref{chapter:epinerf}\\
            We then turn from camera pose encoding to inner 3D constraints consideration in \ac{NeRF}. However, we maintain the epipolar geometry concept in our work and build a feature-based attention mechanism thanks to \ac{NeRF}-based additional network, called \textit{NeRFeature}. Such a mecanism is direclty involved at training time, while we \textit{do not} have access to the target view to build epipolar constraints. Our work is currently under review at 3DV:
            \begin{itemize}
                  \item \fullcite{landreau2024epinerf}
            \end{itemize}

      \item \autoref{chapter:gausssplat}: \nameref{chapter:gausssplat}\\
            We relaxed the main hypothesis we dealt with in these first two apparoches to work with \ac{NVS} in a multiple-images scenario with 3D \ac{GS}. Such work mostly rely on \textit{CarCutter} 
industrial problematics, as the next generation of the current 360\degree spin stabilization. However, if rendering at training locations goes fine, stabilizing camera path to render unobserved viewpoint, and thus create seamless 360\degree car animation. 

      \item \autoref{chapter:appendix-adaptativeSR}: \nameref{chapter:appendix-adaptativeSR} \\
      The first tree contributions were extensively focused around \ac{NVS}. This fourth and last work is more related to 3D reconstruction issues but remains closed to the original issue this thesis is tackling. It adressed topological consideration onto 3D meshes produced by a single-image 3D reconstruction neural network. We built a module, called \textit{AdaptativeSR}, that prunes mesh faces by solely accounting for 2D silhouette masks rendering. The work in this annex has led to the following conference publication:
      \begin{itemize}
            \item \fullcite{landreau2022adaptativesr}
      \end{itemize}
\end{itemize}


