\chapter{Conclusion}
\label{chapter:conclusion}

%\minitoc
\chapterwithfigures{\nameref*{chapter:conclusion}}
\chapterwithtables{\nameref*{chapter:introduction}}

\ifthenelse{\boolean{skipConclusion}}{\endinput}{}

This thesis primarily focuses on the \ac{NVS} problem, especially in the the highly constrained single-view scenario. We briefly review our core contributions before outlining the current landscape in \ac{AI} and 3D in this conclusion chapter. The final section of this manuscript is devoted to the perspectives and potential further work that may stem from this PhD thesis.  

\section{Summary}

This manuscript has adressed, to a large extent, the single-image \ac{NVS} problem. Our first two contributions were built around deep learning architectures that synthesize a novel viewpoint of static scenes from a single posed image. Our latest contribution has a primary industrial aim, where novel views are rendered from a scene explicitly reconstructed in 3D as a Gaussian point cloud.

We began this manuscript by presenting our EpipolarNVS architecture in Chapter \ref{chapter:epipolarnvs}. Our main motivation for this work was to propose an innovative way to encode camera pose information for an image-to-image \ac{CNN} devoted to single-view \ac{NVS}. While most prior work often discretized \citep{kim2020novel} or encoded camera pose information into a low dimensional signal \citep{sun2018multiview}, we instead exploited the epipolar constraint to encode such prior information. Through a vanilla grid sampling strategy on the source view, we get colored epipolar lines that we stored as an RGB image, fed alongside the source image to pose-condition the \ac{NN}. 

In Chapter \ref{chapter:epinerf}, we investigated how epipolar constraints could be integrated into a generalizable \ac{NeRF} architecture for single-image \ac{NVS}. Based on a \textit{source-aligned} dense feature volume produced from a \ac{CNN}-encoder, we trained a novel \ac{NeRF}-architecture, termed NeRFeature, to produce \textit{target-aligned} features. These \textit{source} and \textit{target-aligned} feature are ultimately used through an epipolar constraint in a light attention mechanism. We extensively shown in the Experiments section \ref{subsec:epinerf/experiments} how our multi-stage training architecture might help generalizable \ac{NeRF} to better perfrom on single-image \ac{NVS} tasks.  

Chapter \ref{chapter:gausssplat} was finally devoted to the \ac{NVS} solution we began investigated with Meero. While research and open-source projects around \ac{GS} \citep{kerbl20233d} are tremendously prolific and progress steadily \citep{luiten2023dynamic,yang2024gaussianobject,wewer24latentsplat}, we presented several improvements to extend beyond the original 3D\ac{GS} work. A denser point cloud using the visual hull concept was introduced at the initialization stage, Gaussians opacity was made view-dependant through an additional \ac{MLP}, allowing for better rendering of transparent surfaces, and a novel densification strategy was implemented. However, 3D\ac{GS}-based scenes remain prone to artifacts, as discussed in section \ref{sec:gs-limitation}, especially when rendered at non-training locations. Further development will therefore focus on improving novel view synthesis at these \textit{out-of-distribution} locations. The introduction of geometric priors and constraints during training (such as depth loss and normals surface regularization) is also curently under development. 

In addition to these contributions, we introduced AdaptativeSR in Appendix \ref{chapter:appendix-adaptativeSR}, an approach that considers topological issues in single-image 3D reconstruction. This module aims to refine the topology of a $genus-0$ mesh by relying solely on 2D silhouette masks and the differentiable renderer from \citep{liu2019soft}. While this contribution is more closely related to 3D reconstruction than \ac{NVS}, this work, with Chapter \ref{chapter:gausssplat} as well, creates a meaningful connection between the two concepts. 

\section{Perspectives and further work}
We present in this section a few perspectives and suggest further research directions that could be explored following this PhD thesis. We have made every effort to draw parallels and connections between current 3D \ac{AI} research and industry. 

\subsection{Further work}
This thesis tried to integrate and leverage on geometric priors and constraints whithin \ac{DL} architectures devoted to \ac{NVS} and 3D reconstruction. When we first began exploring epipolar considerations years ago, few works has attempted to do so. \ac{NVS} was initially addressed through an image-to-image \ac{CNN}, disregarding the physical image formation process and its inherant 3D/2D projection. Our second contribution moves closer to the 3D domain by developing a feature epipolar-based attention mechanism within a generalizable \ac{NeRF} architecture. Our lastest work around 3D\ac{GS} bridges the gap between 3D reconstruction and \ac{NVS} since the learned 3D gaussian cloud is used as an intermediate representation to render novel views at stabilized locations. We present below a few directions and specific topics in these areas that could be addressed in future work.

\noindent \textbf{Single-image 3D reconstruction.} 3D \ac{AI} research is currently extremely prolific in single-image 3D reconstruction (and thus inherently \ac{NVS}). Within the last year, at least three powerful and solid projects have been released:  One-2-3-45++ \citep{liu2023one2345++} in November 2023 ,TripoSR \citep{tochilkin2024triposr} in March 2024 and InstantMesh \citep{xu2024instantmesh} in April 2024. The former two were outperformed by the latest InstantMesh. While \ac{NVS} may no longer be addressed in the same way in the future, exploring single-image 3D reconstruction remains a challenging yet intringuing research path. To the best of our knowledge, none of these works attempts to integrate symmetry priors or epipolar constraints into their architectures. 

% mixing diffusion and 3D for single-image NVS
\noindent \textbf{Diffusion and 3D for single-image NVS.} While we extensively relied on \ac{NeRF} attributes in our EpiNeRF contribution, the latest advances in diffusion-based models enable single-image \ac{NVS} to be adressed under the same assumptions as ours: a unique source viewpoint and a rich, diverse dataset as ShapeNet \citep{chang2015shapenet}. NeRFDiff \citep{gu2023nerfdiff}, Diffrf \citep{muller2023diffrf}, and GeNVS \citep{chan2023genvs} combine both 3D radiance fields with large diffusion models to tackle such an issue. 

However, these architectures remain complex to train, and rendering time requirements prevents real-time inference. These works are also mostly developed by private compagnies that do not release their source code. In this way, NeRFDiff was partially developed by Apple and GeNVS by Nvidia, but neither has made their code publicly available. Current research therefore suffers from the lack of open-source single-image \ac{NVS} code\footnote{We exclude recent single-image 3D reconstruction methods here, as they cannot handle complex and large datasets that are not object centric, such as CO3D \citep{reizenstein2021common} or RealEstate10K \citep{zhou2018stereo}.}. It directly relates to the open-source issue discussed in section \ref{sec:conclusion-open} of this conclusion. It would be worthwhile to pursue research on how diffusion-based models and \ac{NeRF}/\ac{GS}-based \textit{3D explicitness} could coexist and benefit from one another in a unified \ac{NVS} architecture.

While \ac{NeRF} suffers from the millions of queries needed to render an image, the current interest in 3D\ac{GS}-based technology, along with rapid advances in diffusion models, should allow the design of an effective yet fast single-image \ac{NVS} architecture.

%
\noindent \textbf{Generalizable 3DGS for high resolution single-image NVS.} Finally, one could explore a generalizable 3D\ac{GS}-based framework for high-resolution single-image \ac{NVS}. While generalizable \ac{NeRF} architectures have existed for a few years now \citep{yu2021pixelnerf,li2022symmnerf,lin2023vision}, image resolutions (ranging from $64\times 64$ to $1024\times 1024$ pixels) and rendering speed are still too low for an industrial purpose. Research around 3D\ac{GS} currently seems to follow a similar path to the one \ac{NeRF} followed. The weaknesss of \ac{GS} identified in the seminal paper \citep{kerbl20233d} are thus subject to a plethora of works over the last few months: regarding COLMAP dependencies for camera poses \citep{fu2023colmapfree}, the number of source images used \citep{xiong2023sparsegs,yang2024gaussianobject}, ways to reduce model weight storage \citep{niedermayr2023compressed}, techniques to speed up training and inference time \citep{fan2024instantsplat} or improve rendering quality \citep{yu2023mip}, or even extend the original 3D\ac{GS} work to dynamic scenes \citep{luiten2023dynamic} or get considerations for graphics attributes \citep{wu2024deferredgs}.  

However, perserving quality and rendering performances of 3D\ac{GS} scenes for highly diverse datasets, as we did with \ac{NeRF} \citep{landreau2024epinerf} and ShapeNet \citep{chang2015shapenet} data, has been barely addressed over the past few months. Some works, such as pixelSplat \citep{charatan23pixelsplat} and latentSplat \citep{wewer24latentsplat} have positioned in such a direction, but they still required two images as input to perform the novel viewpoint generation, whereas image resolution remains limited to $256\times 256$. Therefore, one could design a method to properly image-condition a generalizable 3D\ac{GS} architecture. This should ultimately lead to an increase in the number of learnable attributes for each Gaussian primitive, as research on dynamics scene has done with 4D Gaussians primitives \citep{luiten2023dynamic,gao2024gaussianflow}.  

\subsection{Trends and applications in 3D}
\ac{AI} applications to industry has undergone a significant revolution since 2022, mostly in content generation from both a vision and \ac{LLM} perspective. Recent breakthroughs in \ac{LLM} and diffusion-based models over the past two years have led to the emergence of a remarkable number of startups. 

While all application domains are impacted through rapid advancements of \ac{GenAI} models, such a term primarily refers to text and image generative \ac{AI} models. Applications involving an additional dimension --whether time, as in video, or space, as in 3D-- remain relatively under-addressed in industry. However, an increasing number of companies, such as OpenAI with \textit{SoRA} or LumaAI, are currently releasing products focused on video and 3D. The 3D domain, in its broadest sense, is poised for significant growth in the industry over the coming months and years. In this section, we review some of the latest trends and upcoming applications for the 3D-\ac{AI} landscape. 

\noindent \textbf{Immersive web interaction.} There are numerous domains where online 3D web visualization could significantly enhance the user experience, and the e-commerce is one of them. Being able to smoothly and thoroughly inspect every angle of a pair of shoes on a marketplace or a car on a dealership website is a feature that will soon be common for all websites. The same concept applies to fashion industry, where 3D virtual try-on is gradually gaining importance. However, nearly all current virtual try-on solutions rely on 2D diffusion-based models. As the third dimension is often overlooked, generated images for virtual try-on frequently lack detail or structural coherence, especially with complex poses. Leveraging the inherent 3D structure of objects to develop industrial products is a promising avenue, currently being explored by companies such as Graswald, WANNA, and Meero/CarCutter. 

\noindent \textbf{Biology and medical.} NVIDIA CEO Jensen Huang rencently predicted that digital biology will be "one of biggest revolutions ever". Digital biology spans a wide range of fields, including genomics, proteomics or cell engineering. Biology is on the verge of becoming digitized, allowing it to be approached as an engineering problem, where \ac{AI}, computer science, and big data converge. Since 1990, computational genetics has made remarkable strides, following the Human Genome Project, which required 13 years and a \$3.8 billion investment to sequence 92\% of the human genome. The associated AlphaFoldDB is a massive collection of 200 million predicted protein structures. It continues to aid academics and industries, such as Owkin, in leveraging \ac{AI} to better understand viruses and cell behavior. This advancement aims to develop new cancer treatments, vaccines, and accelerate clinical trials. Additionally, medical imaging and 3D CT scans are poised to benefit from the latest advances in 3D \ac{AI}. In April 2024, the world's most powerful \ac{MRI} scanner, Iseult, led by the CEA, unveilved its very first results. The brain images produced by this 11.7 Tesla \ac{MRI} machine have reached an unprecedented resolution, as depicted in Figure \ref{fig:conclusion-ceaiseult}. The extremely high-resolution images made possible by this MRI machine, combined with the latest \ac{AI} advances, are set to revolutionize medical imaging and 3D organ reconstruction in the near future.

\begin{figure}[htb!]
  \center
\includegraphics[width=\linewidth]{images/conclusion/cea-iseult.jpg}
\caption{\textbf{Axial sections of the human brain.} Most common \ac{MRI} machines in hospitals have a magnetic field strength of 3 Tesla, producing brain images like the one on the far left. While only a few \ac{MRI} machines worldwide reach 7 Tesla, the latest CEA Iseult \ac{MRI}, with its unique 11.7 Tesla magnetic field, delivers exceptional results without any increase in exposure time.}
\label{fig:conclusion-ceaiseult}
\end{figure}

\noindent \textbf{VR/AR and LiDAR.} \ac{VR} and \ac{AR} have a long-standing history spanning 50 years. Recent advances in \ac{NeRF} have paved the way for \ac{VR}-oriented developments, such as VR-NeRF \citep{xu2023vr}, which enables real-time dual 2K generation in walkable virtual reality spaces. Apple's VisionPro mixed-reality headset, released in 2023 and marketed as a \textit{spatial computer}, is expected to have a major impact on how \ac{VR} and \ac{AR} are consumed on a daily basis by individuals. Its associated operating system, termed \textit{visionOS}, will allow developers and researchers to create 3D and \ac{CGI}-based applications on this new spatial computing platform.

\ac{LiDAR} sensors are now integrated into nearly all flagship smartphones, opening up vast opportunities for research and development in highly detailed 3D reconstruction. The China-based startup XGRID is working on a \ac{LiDAR}, multi-\ac{SLAM} and 3D\ac{GS}-based solution to build realistic large-scale 3D scenes, potentially at street or city level in the near future. These ultra-realistic scenes could then be processed using \ac{AI} and conventional graphics and game engines, such as Blender, Maya or UnrealEngine. 

\noindent \textbf{Animation and video games.} Finally, animation and video games industries are about to undergo a significant revolution. Processes that have been in place for decades to animate 3D caracters or render highly detailed \ac{CGI} scenes are about to be disrupted by \ac{AI}. From storyboarding to 3D modeling and animation, creating a high-quality cartoon or animated film typically takes months and even years and involves dozens of skilled roles. \ac{AI} and 3D will streamline these costly production piplines. Recent advances in 3D meshes and 3D\ac{GS}-based rigging and animation \citep{qian2023gaussianavatars,li2024animatablegaussians} confirms such a claim. Both animation and video production are set to experience drastic changes in a close future. 

\ac{CGI}, widely used in the film industry, will also see major advancements thanks to the latest research in neural rendering and graphics. 3D\ac{GS}-based methods designed to address challenges in texture, shading and lightning have already been published \citep{jiang2023gaussianshader,wu2024deferredgs}, and 3D\ac{GS} scene can even be manually corrected and modified using \href{https://github.com/aras-p/UnityGaussianSplatting/}{Unity UnrealEngine} add-on. 


\section{IA challenges in 2024}

This section embrasses the most recent issues and trends currently shaping the 3D-related \ac{AI} landscape. It is not technical in nature but aims to cover the main challenges that the scientific community —from academic researchers to Tech companies— and individuals will face in the months and years ahead.

\subsection{Open-source}
\label{sec:conclusion-open}
\ac{AI} has made its way into world with the release of ChatGPT by OpenAI in December 2022, an \ac{LLM}-based conversational chatbot. 

However, the underlying technology of \ac{LLM} was already well understood by scientific community years before its release. OpenAI was used to publish both code and technical notes on their latest advancements in their \textit{GPT} models. The GPT-2 model, a predecessor to ChatGPT, was notably permissive from a licencing perspective in 2018. However, this period of openness has since ended at OpenAI. The company now only releases source code for a few of its products, such as \textit{Whisper}, a general-purpose speech recognition model. The \ac{AI} community has witnessed a fundamental schism in the way research and code are shared or kept secret. It puts back to back compagnies advocating for an open-source access to significant advancements in \ac{AI} against those arguing for the necessity to keep proprietary control over technologies and \ac{AI}-based algorithms, that could be misused. MistralAI, HuggingFace, StabilityAI and even Meta now stand in opposition to companies like OpenAI, Anthropic, and MidJourney, which market and distribute \ac{AI} products with technological foundations that remain inaccessible to academic researchers and individuals. 

I firmly believe that \ac{AI} research is now supported by a growing community of experts who are highly motivated by the forthcoming technological and intellectual challenges \ac{AI} is raising. Closed-proprietary \ac{AI} companies will ineluctably be overtaken by their open-source counterparts. Figure \ref{fig:conclusion-openclose} illustrates this claim for \ac{LLM}, where open-source dynamics is stronger that its closed-source counterpart. 

\begin{figure}[htb!]
    \center
  \includegraphics[width=\linewidth]{images/conclusion/open-close.jpeg}
  \caption{\textbf{LLMs ELO score based on human ratings.} Such a graph is updated on a regular monthly basis, thanks to \citep{chiang2024chatbot} work. It appears that open models now have a 6 to 10-months lag, compared to over a year when OpenAI's GPT-4 model was released.}
  \label{fig:conclusion-openclose}
\end{figure}

While 3D \ac{AI} is not as prominently affected by such a schism as \ac{LLM}, we are currently observing an increasing number of startups raising funds to support one of the two models. StabilityAI, an already esteemed firm in the \ac{AI} landscape for its diffusion models \citep{esser2024scaling}, has recently made public a significant number of single-image 3D reconstruction models \citep{tochilkin2024triposr,voleti2024sv3d}. 

In the realm of 3D reconstruction and \ac{NVS} through 3D\ac{GS} considerations, the open-source framework \href{https://docs.gsplat.studio/main/}{gsplat} benefits from daily updates by the community. The code we are currently developing at Meero is also based on this commercially permissive library. 

\subsection{Fundation models and environmental issue}

Almost all modalities, from text (\textit{Mistral7B} by MistralAI, \textit{Llama2} by Meta, \textit{Claude3} by Antropic, and \textit{GPT-4} by OpenAI) to image (\textit{Stable Diffusion 3} by StabilityAI, \textit{DALL.E 3} by OpenAI, Midjourney \textit{V6}), as well as video (\textit{SoRA} by OpenAI, \textit{Gen-2} by Runway), now have their fundation models. The 3D domain, along with audio, may be among the last areas to not have reached this milestone \footnote{We exclude here the latest work in single-image 3D reconstruction, as it is still limited to centric object at low resolution.}. However, domains related to \ac{VR}, video games, or animation have not yet witnessed the \ac{AI} revolution that text and image have experienced over the recent years. OpenAI CEO Sam Altman recently tweeted in April 2024: "movies are going to become video games and video games are going to become something unimaginably better". It's thus hard not to recognize the growing importance that 3D will have in the coming years.

However, while an increasing numbers of models are now freely available, the \ac{GPU} ressources required to train or even perform inference with these models (which contain hundreds of millions to tens of billions of learnable parameters) remain substantial. One of the latest MistralAI model, called \textit{8}$\times$\textit{7B}, requires almost 700GB of vRAM \ac{GPU} to be trained from scratch (an 8-A100 \ac{GPU}s cluster would not be sufficient here) as shown on Table \ref{tab:gs-mistral_requirement}. 

\begin{table}[htp!]
  \caption{\textbf{ Mistral 8x7B model} Half and full precision vRAM \ac{GPU} requirements for training. Figures come from the \href{https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator}{HuggingFace Model Memory Calculator}}
  \label{tab:gs-mistral_requirement}
  \centering
  \begin{adjustbox}{width=\linewidth}
  \begin{tabular}[h]{c||cccc}
  \hline 
   Precision &  \textit{Model} & \textit{Gradient calculation} & \textit{Backward pass} & \textit{Optimizer step} \\
  \hline 
  \textbf{float16} \textit{(half)} & 174.4GB  & 261.7GB & 348.9GB & 348.9GB\\
  \textbf{float32} \textit{(full)} &  174.4GB  & 174.4GB & 348.9GB & 697.9GB \\
  \hline 
  \end{tabular}
  \end{adjustbox}
\end{table}
Factorial Funds estimates in one of its \href{https://www.factorialfunds.com/blog/under-the-hood-how-openai-s-sora-model-works}{reports} that between $4.2K$ and $10K$ Nvidia H100 \ac{GPU}s are required for a whole month to train the latest text-to-video OpenAI \textit{SoRa} models. With a power consumption of 700W  per \ac{GPU}, the training of \textit{SoRA} has consummed as much as electricity in one month as a French individual does in 12 years. The same report also states that a single H100 can produce a 5-minute video in about an hour. If such innovative technology becomes publicly available in the coming months and years, Factorial Funds estimates that OpenAI will require approximately 720,000 H100 \ac{GPU} to meet global demand\footnote{Based on 50\% \ac{AI}-generated videos on TikTok for a daily basis and 15\% on Youtube.}. 

Finally, the current trend from NVDIA regarding its \ac{GPU} development is not geared toward energy efficiency. The primary goal is to continuously push toward higher FLOPS performance: while the next \ac{GPU} Blackwell generation, called B100 and B200, is expected to be 25 times faster than the H100, they will also have a power requirement of 1kW. 


\subsection{AI Ethics} 

Over the past five years, \ac{AI} has made significant progress at a stunning pace. As \ac{AI} capabalities strengthen every day, so does the need to tackle its associated ethical dilemmas. 

Ethics is one of the main arguments put forth by companies advocating for exclusive ownership of their \ac{AI} algorithms and products. They aim to ensure that source codes remain out of the hands of those who might misuse them for malicious reasons. While \ac{AI}-based algorithms for audio or images were insufficiently developed a few years ago to deceive the human eye or ear, deck has been thoroughly reshuffled lately. However, \ac{AI} is not limited to generative content creation; decision-making granted by \ac{AI} are increasingly being adopted in various fields, including medical diagnostics for cancer detection (\eg Owkin) and  more controversial areas such as military applications (\eg Anduril).

In March 2024, OpenAI introduced one of its latest products, the \href{https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices}{Voice Engine}. A 15-second audio recording is now sufficient to clone a human voice with remarkable accuracy using such an \ac{AI}. While OpenAI emphasizes the positive potentials offered by their technology (for live translation or assisting individuals with oral disorders), the company also conceeds that its product brings significant threats, including telephone frauds or identity theft. 

The first deepfakes, which emerged in 2017, originally referred to videos in which a person's face was artificially altered by \ac{AI} to assume the appearance of someone else. Since then, the definition has broadened considerably. In April 2024, Microsoft released VASA-1 \citep{xu2024vasa1}, which enables the generation of voiced deepfakes from a single image. Similar to OpenAI, Microsoft has already communicated the potential threats and misuses that VASA-1 could entail, although no public release has been announced yet.

In mid-2023, Google developed SynthID, a tool that enables for watermarking \ac{AI}-generated image and audio. This indirectly helps to protect original human content creation. Decision-making political authorities and parliaments worldwide are begining to urge their partners to regulate \ac{AI} and establish a coherent legal framework. The AI Act was ratified in Europe in February 2024 and is set to take effect in 2025. It will impose certain transparency requirements regarding the training data used for foundation models and bias studies related to the outcomes produced by these \ac{AI} models. As of now, the US government does not have such a similiar AI Act. 


